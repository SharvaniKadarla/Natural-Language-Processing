{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###**Project Description:**\n",
        "Perform sentiment analysis on three types of text:\n",
        "1. Movie reviews\n",
        "2. Product reviews\n",
        "3. Tweets\n",
        "\n",
        "Two systems are implemented:\n",
        "1. A classical machine learning sentiment classifier\n",
        "2. A transformer-based sentiment classifier using BERT\n",
        "\n",
        "The goal is to compare traditional feature-based models with modern\n",
        "transformer models using the same datasets and evaluation metrics.\n"
      ],
      "metadata": {
        "id": "JbOiWDk-kQze"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dXhk_D72HIaF"
      },
      "outputs": [],
      "source": [
        "# !pip install scikit-learn pandas nltk transformers datasets (Install it initially for once)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "# !pip install -q datasets transformers scikit-learn pandas nltk (Install it initially for once)"
      ],
      "metadata": {
        "id": "kCuQK6EkMFEg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from transformers import pipeline\n",
        "import random"
      ],
      "metadata": {
        "id": "pliw_lWIg992"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK downloads\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4BIOLWZhH6S",
        "outputId": "c48bc13e-b343-4ced-f744-bbcfc57dfc62"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Datasets Used**\n",
        "\n",
        "Three publicly available datasets were used in this project:\n",
        "\n",
        "- IMDB movie reviews for sentiment classification\n",
        "- Amazon Polarity dataset for product reviews\n",
        "- Sentiment140 dataset for Twitter sentiment analysis\n",
        "\n",
        "Each dataset contains positive and negative sentiment labels.\n",
        "A balanced subset of 100 samples was selected from each dataset\n",
        "to ensure fair and efficient experimentation."
      ],
      "metadata": {
        "id": "o-2W8pi8kbVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Dataset Load\n",
        "# Movie reviews (IMDB)\n",
        "imdb = load_dataset(\"imdb\")\n",
        "df_movie = pd.DataFrame(imdb['train'].shuffle(seed=42).select(range(100)))[[\"text\",\"label\"]].head(100)\n",
        "# Mapping labels 0->negative, 1->positive\n",
        "df_movie['label'] = df_movie['label'].map({0: 'negative', 1: 'positive'})"
      ],
      "metadata": {
        "id": "CwV7xO_phMQ4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Product reviews (Amazon Polarity) - Amazon Reviews\n",
        "amazon = load_dataset(\"amazon_polarity\")\n",
        "df_product = pd.DataFrame(amazon['train'].shuffle(seed=42).select(range(100)))[[\"content\",\"label\"]].head(100)\n",
        "df_product.rename(columns={\"content\":\"text\"}, inplace=True)\n",
        "df_product['label'] = df_product['label'].map({0: 'negative', 1: 'positive'})"
      ],
      "metadata": {
        "id": "LQ8hppSIhOUK"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tweets (Sentiment140 or Twitter US Airline Sentiment): Can use any of the dataset\n",
        "\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "import os\n",
        "\n",
        "# Loading the Sentiment140 dataset\n",
        "# Dataset Source Link: https://www.kaggle.com/datasets/kazanova/sentiment140\n",
        "dataset_path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
        "print(\"Dataset downloaded to:\", dataset_path)\n",
        "\n",
        "# The CSV file inside is usually named \"training.1600000.processed.noemoticon.csv\"\n",
        "csv_file = os.path.join(dataset_path, \"training.1600000.processed.noemoticon.csv\")\n",
        "\n",
        "# Loading the CSV File into pandas\n",
        "df_tweets = pd.read_csv(csv_file, encoding='latin-1', names=[\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"])\n",
        "\n",
        "# Filtering only positive (4) and negative (0) tweets with labels\n",
        "df_tweets = df_tweets[df_tweets['target'].isin([0, 4])]\n",
        "\n",
        "# 100 tweets sample\n",
        "neg = df_tweets[df_tweets[\"target\"] == 0].sample(50, random_state=42)\n",
        "pos = df_tweets[df_tweets[\"target\"] == 4].sample(50, random_state=42)\n",
        "\n",
        "df_tweets = pd.concat([neg, pos]).sample(frac=1, random_state=42)\n",
        "df_tweets[\"label\"] = df_tweets[\"target\"].map({0: \"negative\", 4: \"positive\"})\n",
        "df_tweets = df_tweets[[\"text\", \"label\"]]\n",
        "\n",
        "print(\"Tweets dataset loaded. Sample:\")\n",
        "print(df_tweets.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj-fUXJ_wMCo",
        "outputId": "cb1fed7a-befd-4ab6-f845-f4969000d1c9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'sentiment140' dataset.\n",
            "Dataset downloaded to: /kaggle/input/sentiment140\n",
            "Tweets dataset loaded. Sample:\n",
            "                                                      text     label\n",
            "1534779                   @saturnboy good job!  So pretty   positive\n",
            "1388988  @hillsongunited can't wait to worship with you...  positive\n",
            "1403776  Of course, as always, Jerry wants some tweaks ...  positive\n",
            "722634   one month ago i was the most happiest person.....  negative\n",
            "325297   once the concert stories start rollin in I mig...  negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = {\n",
        "    \"Movie\": df_movie,\n",
        "    \"Product\": df_product,\n",
        "    \"Tweets\": df_tweets\n",
        "}"
      ],
      "metadata": {
        "id": "11UVWpfMhSf3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Preprocessing**\n",
        "\n",
        "Text preprocessing is applied only for the classical machine learning models.\n",
        "\n",
        "Steps include:\n",
        "- Converting text to lowercase for consistency\n",
        "- Tokenizing text into individual words\n",
        "- Removing stopwords to reduce noise\n",
        "- Applying lemmatization to normalize word forms\n",
        "\n",
        "These steps help improve model performance by reducing irrelevant variations."
      ],
      "metadata": {
        "id": "QMge1vRuksN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Preprocessing (for Classical ML Sentiment Classifier)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(tok) for tok in tokens if tok.isalpha() and tok not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "for name, df in datasets.items():\n",
        "    df['clean_text'] = df['text'].astype(str).apply(preprocess)"
      ],
      "metadata": {
        "id": "xUxZMLMihg24"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classical Machine Learning Models**\n",
        "\n",
        "Logistic Regression and Naive Bayes are used as classical sentiment classifiers.\n",
        "These models rely on manually engineered features such as TF-IDF vectors.\n",
        "\n",
        "Both models are trained on the same training set and evaluated on a held-out test set.\n",
        "\n",
        "**Note: Feature Engineering**\n",
        "\n",
        "TF-IDF (Term Frequency–Inverse Document Frequency) is used to convert text into numerical feature vectors.\n",
        "\n",
        "TF-IDF assigns higher importance to words that are meaningful within a document while reducing the influence of commonly occurring words.\n",
        "The vocabulary size is printed to understand the dimensionality of the feature space.\n"
      ],
      "metadata": {
        "id": "fEV_kBR8lGw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Classical ML Sentiment Classifier: Vectorize + Train + Evaluate (System A)\n",
        "results = {}\n",
        "test_sets = {}\n",
        "\n",
        "for name, df in datasets.items():\n",
        "    if df.empty:\n",
        "        continue\n",
        "    print(f\"\\n===== {name} Dataset (Classical ML) =====\")\n",
        "    X = df['clean_text']\n",
        "    y = df['label']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    test_sets[name] = (X_test, y_test)\n",
        "\n",
        "    vectorizer = TfidfVectorizer()  # or CountVectorizer() for Bag-of-Words\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "    print(\"Vocabulary size:\", len(vectorizer.vocabulary_))\n",
        "\n",
        "    # Logistic Regression Classifier\n",
        "    clf = LogisticRegression(max_iter=1000)\n",
        "    clf.fit(X_train_vec, y_train)\n",
        "    y_pred = clf.predict(X_test_vec)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, pos_label='positive')\n",
        "    rec = recall_score(y_test, y_pred, pos_label='positive')\n",
        "    f1 = f1_score(y_test, y_pred, pos_label='positive')\n",
        "    print(\"\\nLogistic Regression Classifier Results:\")\n",
        "    print(\"Accuracy:\", acc)\n",
        "    print(\"Precision:\", prec)\n",
        "    print(\"Recall:\", rec)\n",
        "    print(\"F1‑score:\", f1)\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "    # Naive Bayes Classifier\n",
        "    nb = MultinomialNB()\n",
        "    nb.fit(X_train_vec, y_train)\n",
        "    nb_pred = nb.predict(X_test_vec)\n",
        "\n",
        "    nb_acc = accuracy_score(y_test, nb_pred)\n",
        "    nb_prec = precision_score(y_test, nb_pred, pos_label='positive')\n",
        "    nb_rec = recall_score(y_test, nb_pred, pos_label='positive')\n",
        "    nb_f1 = f1_score(y_test, nb_pred, pos_label='positive')\n",
        "\n",
        "    print(\"\\nNaive Bayes Classifier Results:\")\n",
        "    print(\"Accuracy:\", nb_acc)\n",
        "    print(\"Precision:\", nb_prec)\n",
        "    print(\"Recall:\", nb_rec)\n",
        "    print(\"F1-score:\", nb_f1)\n",
        "\n",
        "    # Results\n",
        "    results[name + \"_LR\"] = {\n",
        "    \"model\": \"Logistic Regression\",\n",
        "    \"accuracy\": acc,\n",
        "    \"precision\": prec,\n",
        "    \"recall\": rec,\n",
        "    \"f1\": f1}\n",
        "\n",
        "    results[name + \"_NB\"] = {\n",
        "    \"model\": \"Naive Bayes\",\n",
        "    \"accuracy\": nb_acc,\n",
        "    \"precision\": nb_prec,\n",
        "    \"recall\": nb_rec,\n",
        "    \"f1\": nb_f1}\n",
        "\n",
        "    print(\"\\nSome test samples with predictions:\")\n",
        "    for i, text in enumerate(X_test.iloc[:5]):\n",
        "        print(\"Text:\", text)\n",
        "        print(\"True label:\", y_test.iloc[i], \"Pred:\", y_pred[i])\n",
        "        print(\"----\")\n",
        "\n",
        "    print(\"\\nExample Correct vs Incorrect Predictions:\")\n",
        "\n",
        "    correct_example = None\n",
        "    incorrect_example = None\n",
        "\n",
        "    for i in range(len(y_test)):\n",
        "        if y_test.iloc[i] == y_pred[i] and correct_example is None:\n",
        "            correct_example = i\n",
        "        if y_test.iloc[i] != y_pred[i] and incorrect_example is None:\n",
        "            incorrect_example = i\n",
        "        if correct_example is not None and incorrect_example is not None:\n",
        "            break\n",
        "\n",
        "    if correct_example is not None:\n",
        "        print(\"\\nCorrect Prediction Example:\")\n",
        "        print(\"Text:\", X_test.iloc[correct_example][:200])\n",
        "        print(\"True Label:\", y_test.iloc[correct_example])\n",
        "        print(\"Predicted Label:\", y_pred[correct_example])\n",
        "\n",
        "    if incorrect_example is not None:\n",
        "        print(\"\\nIncorrect Prediction Example:\")\n",
        "        print(\"Text:\", X_test.iloc[incorrect_example][:200])\n",
        "        print(\"True Label:\", y_test.iloc[incorrect_example])\n",
        "        print(\"Predicted Label:\", y_pred[incorrect_example])"
      ],
      "metadata": {
        "id": "bZ9a5_kuhkmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c9dceaf-ce06-46b4-a7f6-c4fae12a8e12"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Movie Dataset (Classical ML) =====\n",
            "Vocabulary size: 3796\n",
            "\n",
            "Logistic Regression Classifier Results:\n",
            "Accuracy: 0.65\n",
            "Precision: 1.0\n",
            "Recall: 0.2222222222222222\n",
            "F1‑score: 0.36363636363636365\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.61      1.00      0.76        11\n",
            "    positive       1.00      0.22      0.36         9\n",
            "\n",
            "    accuracy                           0.65        20\n",
            "   macro avg       0.81      0.61      0.56        20\n",
            "weighted avg       0.79      0.65      0.58        20\n",
            "\n",
            "\n",
            "Naive Bayes Classifier Results:\n",
            "Accuracy: 0.6\n",
            "Precision: 1.0\n",
            "Recall: 0.1111111111111111\n",
            "F1-score: 0.2\n",
            "\n",
            "Some test samples with predictions:\n",
            "Text: gu van sant made excellent film truly br br however ca help feel cerebral edge tom robbins book even cowgirl get blue lost translation big screen alone tom robbins gu van sant incredible visionary tower talent ultimately though one work br br character well developed plot content come alive imagination much powerful reading book like taken away different time place sometimes think worst best add overall book neatly unfolds according author precision movie however leave one less imagination emotion detracting overall experience believe happened br br suggest reading book\n",
            "True label: positive Pred: negative\n",
            "----\n",
            "Text: sister said movie gon na good second thought watched actually funny basically movie made weird girl go small town one like want go get reading aunt go easy movie come across hilarious humor witch book mentally challenged uncle dog understand meaning word freak anyways hope run right try find really old movie hope like total give totally joking ill give hope understand laugh scream may br br love truly dakota email hot wan na\n",
            "True label: positive Pred: negative\n",
            "----\n",
            "Text: unimpressed cinderella jungle book possibly worse title first like animation worse scene liked character namely thunderbolt patch character like cruella mediocre cruella truly villainous original lost quality sequel said nothing write home animation kind ugly also artist companion lars joke honest roger seemed quit smoking overnight voice talent good though especially barry bostwick thunderbolt exception jodi benson accent ruined good moment whole plot seemed bloated highly suggestive extended tv episode hugely disappointing sequel memorable disney movie along jungle book sorry give cup tea bethany cox\n",
            "True label: negative Pred: negative\n",
            "----\n",
            "Text: opinion pretty good celebrity skit show enjoyed seeing greg kinnear host many reason said even though hal spark okay host sometimes wish greg kinnear left ask seems nobody stay tv show throughout entire run anymore still enjoyed seeing various host people spoofing celebrity ask pretty darn funny wrap must say kind miss show conclusion highly recommend show sketch show fan really enjoy\n",
            "True label: positive Pred: negative\n",
            "----\n",
            "Text: earth colin firth pointless film really strapped cash br br film clear want grief exotic place ghost vehicle mr darcy muddled muddy br br seems sort idea italy must good italian something offer language end girl want go back br br pointless episode beach church busy road anybody care simply br br also yank woman film clear job seemed make vapid inappropriate maudlin comment girl supposed paedophillia br br pretty dreadful mess gave rather charm utterly ghastly film\n",
            "True label: negative Pred: negative\n",
            "----\n",
            "\n",
            "Example Correct vs Incorrect Predictions:\n",
            "\n",
            "Correct Prediction Example:\n",
            "Text: unimpressed cinderella jungle book possibly worse title first like animation worse scene liked character namely thunderbolt patch character like cruella mediocre cruella truly villainous original lost\n",
            "True Label: negative\n",
            "Predicted Label: negative\n",
            "\n",
            "Incorrect Prediction Example:\n",
            "Text: gu van sant made excellent film truly br br however ca help feel cerebral edge tom robbins book even cowgirl get blue lost translation big screen alone tom robbins gu van sant incredible visionary tow\n",
            "True Label: positive\n",
            "Predicted Label: negative\n",
            "\n",
            "===== Product Dataset (Classical ML) =====\n",
            "Vocabulary size: 1524\n",
            "\n",
            "Logistic Regression Classifier Results:\n",
            "Accuracy: 0.55\n",
            "Precision: 0.6\n",
            "Recall: 0.3\n",
            "F1‑score: 0.4\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.53      0.80      0.64        10\n",
            "    positive       0.60      0.30      0.40        10\n",
            "\n",
            "    accuracy                           0.55        20\n",
            "   macro avg       0.57      0.55      0.52        20\n",
            "weighted avg       0.57      0.55      0.52        20\n",
            "\n",
            "\n",
            "Naive Bayes Classifier Results:\n",
            "Accuracy: 0.5\n",
            "Precision: 0.5\n",
            "Recall: 0.4\n",
            "F1-score: 0.4444444444444444\n",
            "\n",
            "Some test samples with predictions:\n",
            "Text: unit work well nice long cord good value ear pad bit small\n",
            "True label: positive Pred: negative\n",
            "----\n",
            "Text: son soes like variable nipple tried one take bottle well problem hard find store buy extra found\n",
            "True label: positive Pred: negative\n",
            "----\n",
            "Text: bought iron two year ago forever leaving water even big mistake spit water well heating element work properly iron made take abuse get another however buy another like love titanium finish soleplate ca tell many time used fusible fabric never stuck much better surface used wish could get iron repaired everything else work fine\n",
            "True label: positive Pred: negative\n",
            "----\n",
            "Text: ordered book christmas gift least friend family avid reader least book week usually forget book shortly read book hard forget week later still recall whole chapter touched life much want share many people possible highly recommend year later one book gave friend died unexpectantly talked book many time know room marvel helped deal loss know touched life book\n",
            "True label: positive Pred: positive\n",
            "----\n",
            "Text: given gift soon realized product flawed player connected pc battery dy corrupt internal memory player restart replacing battery player bios initiate however internal memory check complete message battery low appear shuts internal memory player must contacted digitalway regarding said player purchase date less day old receipt may return place purchase player purchase date day less day old receipt may return digitalway key receipt also believe really backing product would recommend looking comparable player especially warranty\n",
            "True label: negative Pred: negative\n",
            "----\n",
            "\n",
            "Example Correct vs Incorrect Predictions:\n",
            "\n",
            "Correct Prediction Example:\n",
            "Text: ordered book christmas gift least friend family avid reader least book week usually forget book shortly read book hard forget week later still recall whole chapter touched life much want share many pe\n",
            "True Label: positive\n",
            "Predicted Label: positive\n",
            "\n",
            "Incorrect Prediction Example:\n",
            "Text: unit work well nice long cord good value ear pad bit small\n",
            "True Label: positive\n",
            "Predicted Label: negative\n",
            "\n",
            "===== Tweets Dataset (Classical ML) =====\n",
            "Vocabulary size: 451\n",
            "\n",
            "Logistic Regression Classifier Results:\n",
            "Accuracy: 0.6\n",
            "Precision: 0.5833333333333334\n",
            "Recall: 0.7\n",
            "F1‑score: 0.6363636363636364\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.62      0.50      0.56        10\n",
            "    positive       0.58      0.70      0.64        10\n",
            "\n",
            "    accuracy                           0.60        20\n",
            "   macro avg       0.60      0.60      0.60        20\n",
            "weighted avg       0.60      0.60      0.60        20\n",
            "\n",
            "\n",
            "Naive Bayes Classifier Results:\n",
            "Accuracy: 0.7\n",
            "Precision: 0.75\n",
            "Recall: 0.6\n",
            "F1-score: 0.6666666666666666\n",
            "\n",
            "Some test samples with predictions:\n",
            "Text: allright good night amp thank much hug\n",
            "True label: positive Pred: negative\n",
            "----\n",
            "Text: mad late good luck\n",
            "True label: positive Pred: positive\n",
            "----\n",
            "Text: aquaahh ilove everything old school\n",
            "True label: positive Pred: positive\n",
            "----\n",
            "Text: hillsongunited ca wait worship guy tonight much fun\n",
            "True label: positive Pred: negative\n",
            "----\n",
            "Text: anniegxxx ca think way least try lol\n",
            "True label: negative Pred: positive\n",
            "----\n",
            "\n",
            "Example Correct vs Incorrect Predictions:\n",
            "\n",
            "Correct Prediction Example:\n",
            "Text: mad late good luck\n",
            "True Label: positive\n",
            "Predicted Label: positive\n",
            "\n",
            "Incorrect Prediction Example:\n",
            "Text: allright good night amp thank much hug\n",
            "True Label: positive\n",
            "Predicted Label: negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example Predictions Analysis:\n",
        "\n",
        "The correctly classified example shows clear sentiment-related keywords, making it easier for the model to predict accurately. The incorrect example highlights cases where sentiment may be subtle,\n",
        "ambiguous, or context-dependent, which classical models often struggle with.\n"
      ],
      "metadata": {
        "id": "gf97wPaIrqxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformer-Based Sentiment Analysis (BERT)**\n",
        "\n",
        "BERT is a transformer-based model that captures contextual meaning in text.\n",
        "\n",
        "The pre-trained DistilBERT model fine-tuned on SST-2 is used via the HuggingFace pipeline.\n",
        "\n",
        "Unlike classical models, BERT does not require manual preprocessing or feature engineering.\n",
        "\n",
        "The model is evaluated on the same test samples used for classical models to ensure fairness.\n"
      ],
      "metadata": {
        "id": "tcQG4x6xlnnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. BERT‑based sentiment (System B: Transformer-Based Sentiment Model) (System B)\n",
        "# BERT - Bidirectional Encoder Representations from Transformers.\n",
        "print(\"\\nTransformer-Based Sentiment Model(BERT)\")\n",
        "bert_clf = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    truncation=True,   # Ensuring texts >512 tokens are truncated\n",
        "    max_length=512\n",
        ")\n",
        "results_bert = {}\n",
        "\n",
        "for name, df in datasets.items():\n",
        "    if df.empty:\n",
        "        continue\n",
        "    print(f\"\\n--- {name} dataset (BERT) ---\")\n",
        "\n",
        "    # Using SAME test set as Classical ML\n",
        "    X_test, y_test = test_sets[name]\n",
        "\n",
        "    texts = X_test.astype(str).tolist()\n",
        "    true_labels = y_test.tolist()\n",
        "\n",
        "    preds = bert_clf(texts)\n",
        "    bert_labels = [pred['label'].lower() for pred in preds]\n",
        "\n",
        "    # Evaluation metrics\n",
        "    acc = accuracy_score(true_labels, bert_labels)\n",
        "    prec = precision_score(true_labels, bert_labels, pos_label='positive')\n",
        "    rec = recall_score(true_labels, bert_labels, pos_label='positive')\n",
        "    f1 = f1_score(true_labels, bert_labels, pos_label='positive')\n",
        "    print(f\"BERT metrics → Accuracy: {acc:.2f}, Precision: {prec:.2f}, Recall: {rec:.2f}, F1-Score: {f1:.2f}\")\n",
        "\n",
        "    results_bert[name] = {\"model\": \"BERT\", \"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
        "\n",
        "    # Sample test predictions\n",
        "    for t, p in zip(texts[:10], preds[:10]):\n",
        "        print(\"Text:\", t[:200], \"...\")  # printing only first 200 chars for readability\n",
        "        print(\"BERT →\", p)\n",
        "        print(\"-----\")"
      ],
      "metadata": {
        "id": "dFRNtMOkhoS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7394740-2bda-47a5-acbb-db36978e27a5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transformer-Based Sentiment Model(BERT)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Movie dataset (BERT) ---\n",
            "BERT metrics → Accuracy: 0.85, Precision: 1.00, Recall: 0.67, F1-Score: 0.80\n",
            "Text: gu van sant made excellent film truly br br however ca help feel cerebral edge tom robbins book even cowgirl get blue lost translation big screen alone tom robbins gu van sant incredible visionary tow ...\n",
            "BERT → {'label': 'POSITIVE', 'score': 0.9903706908226013}\n",
            "-----\n",
            "Text: sister said movie gon na good second thought watched actually funny basically movie made weird girl go small town one like want go get reading aunt go easy movie come across hilarious humor witch book ...\n",
            "BERT → {'label': 'NEGATIVE', 'score': 0.8905219435691833}\n",
            "-----\n",
            "Text: unimpressed cinderella jungle book possibly worse title first like animation worse scene liked character namely thunderbolt patch character like cruella mediocre cruella truly villainous original lost ...\n",
            "BERT → {'label': 'NEGATIVE', 'score': 0.9994425177574158}\n",
            "-----\n",
            "Text: opinion pretty good celebrity skit show enjoyed seeing greg kinnear host many reason said even though hal spark okay host sometimes wish greg kinnear left ask seems nobody stay tv show throughout enti ...\n",
            "BERT → {'label': 'POSITIVE', 'score': 0.9976315498352051}\n",
            "-----\n",
            "Text: earth colin firth pointless film really strapped cash br br film clear want grief exotic place ghost vehicle mr darcy muddled muddy br br seems sort idea italy must good italian something offer langua ...\n",
            "BERT → {'label': 'NEGATIVE', 'score': 0.9980921149253845}\n",
            "-----\n",
            "Text: finished watching movie ridiculously bad really disappointed really sure someone would make movie like marginally entertaining feel like people making lot disagreement making monday writer charge tues ...\n",
            "BERT → {'label': 'NEGATIVE', 'score': 0.9989960789680481}\n",
            "-----\n",
            "Text: someone staggered incredible visuals hero anxious see film billed along line better also featured actress like ziyi zhang well disappointed count bought dvd film mistake br br realize martial art film ...\n",
            "BERT → {'label': 'NEGATIVE', 'score': 0.9925881028175354}\n",
            "-----\n",
            "Text: soprano probably widely acclaimed tv series ever naturally expectation roof yet show surpassed love mafia crime genre film enjoy following compelling story set world much hour material give story chan ...\n",
            "BERT → {'label': 'POSITIVE', 'score': 0.996277391910553}\n",
            "-----\n",
            "Text: think favorite part movie one exemplifies sheer pointless stupidity inanity proceeding come climax film doctor ted nelson unmarried friend sheriff finally cornered melting man landing stair electrical ...\n",
            "BERT → {'label': 'NEGATIVE', 'score': 0.9978529214859009}\n",
            "-----\n",
            "Text: film failed explore humanity animal left empty feeling inside spoiler ahead convinced really compelling reason forego big buyout deal help furry friend whereas babe original bucked trend hit focusing  ...\n",
            "BERT → {'label': 'NEGATIVE', 'score': 0.9983041286468506}\n",
            "-----\n",
            "\n",
            "--- Product dataset (BERT) ---\n",
            "BERT metrics → Accuracy: 0.65, Precision: 0.71, Recall: 0.50, F1-Score: 0.59\n",
            "Text: unit work well nice long cord good value ear pad bit small ...\n",
            "BERT → {'label': 'POSITIVE', 'score': 0.9939022064208984}\n",
            "-----\n",
            "Text: son soes like variable nipple tried one take bottle well problem hard find store buy extra found ...\n",
            "BERT → {'label': 'NEGATIVE', 'score': 0.9971779584884644}\n",
            "-----\n",
            "Text: bought iron two year ago forever leaving water even big mistake spit water well heating element work properly iron made take abuse get another however buy another like love titanium finish soleplate c ...\n",
            "BERT → {'label': 'NEGATIVE', 'score': 0.845703125}\n",
            "-----\n",
            "Text: ordered book christmas gift least friend family avid reader least book week usually forget book shortly read book hard forget week later still recall whole chapter touched life much want share many pe ...\n",
            "BERT → {'label': 'POSITIVE', 'score': 0.9756944179534912}\n",
            "-----\n",
            "Text: given gift soon realized product flawed player connected pc battery dy corrupt internal memory player restart replacing battery player bios initiate however internal memory check complete message batt ...\n",
            "BERT → {'label': 'NEGATIVE', 'score': 0.9973987340927124}\n",
            "-----\n",
            "Text: battery received apc picture description suggested able return ...\n",
            "BERT → {'label': 'NEGATIVE', 'score': 0.9395267963409424}\n",
            "-----\n",
            "Text: sorry say movie disappointment harrison ford best expected better make convincing cowboy indian seemed rather lame movie character well developed beginning story boring movie plot well done decent par ...\n",
            "BERT → {'label': 'NEGATIVE', 'score': 0.9080843329429626}\n",
            "-----\n",
            "Text: looked planer going buy planer amp first concern amp motor planer fail put machine work rough cut white oak blade full bore got tired hitachi planer wish shoe planer class inch longer longer shoe base ...\n",
            "BERT → {'label': 'NEGATIVE', 'score': 0.996464729309082}\n",
            "-----\n",
            "Text: gruen writes like woman trying sound like man word choice syntax ring true unfortunately method cost credibility book nothing write home certainly rank anything erik larson high rating reviewer puzzli ...\n",
            "BERT → {'label': 'POSITIVE', 'score': 0.633484423160553}\n",
            "-----\n",
            "Text: although metal extremely flimsy hold disc securely total waste money ...\n",
            "BERT → {'label': 'NEGATIVE', 'score': 0.998576283454895}\n",
            "-----\n",
            "\n",
            "--- Tweets dataset (BERT) ---\n",
            "BERT metrics → Accuracy: 0.90, Precision: 0.90, Recall: 0.90, F1-Score: 0.90\n",
            "Text: allright good night amp thank much hug ...\n",
            "BERT → {'label': 'POSITIVE', 'score': 0.999849796295166}\n",
            "-----\n",
            "Text: mad late good luck ...\n",
            "BERT → {'label': 'POSITIVE', 'score': 0.9998016953468323}\n",
            "-----\n",
            "Text: aquaahh ilove everything old school ...\n",
            "BERT → {'label': 'NEGATIVE', 'score': 0.9765186905860901}\n",
            "-----\n",
            "Text: hillsongunited ca wait worship guy tonight much fun ...\n",
            "BERT → {'label': 'POSITIVE', 'score': 0.9957802295684814}\n",
            "-----\n",
            "Text: anniegxxx ca think way least try lol ...\n",
            "BERT → {'label': 'NEGATIVE', 'score': 0.9956562519073486}\n",
            "-----\n",
            "Text: amytheallen assume check actually win something ...\n",
            "BERT → {'label': 'NEGATIVE', 'score': 0.9946137070655823}\n",
            "-----\n",
            "Text: xpowxbangxboomx dammiitt wish mtv ...\n",
            "BERT → {'label': 'NEGATIVE', 'score': 0.9869897961616516}\n",
            "-----\n",
            "Text: sillybeggar congrats james sure book going huge success ...\n",
            "BERT → {'label': 'POSITIVE', 'score': 0.999553382396698}\n",
            "-----\n",
            "Text: grounded ...\n",
            "BERT → {'label': 'POSITIVE', 'score': 0.9650882482528687}\n",
            "-----\n",
            "Text: divacandicem candice dont mean bother im really sad u got released ...\n",
            "BERT → {'label': 'NEGATIVE', 'score': 0.9736489653587341}\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary Table\n",
        "summary = pd.DataFrame([\n",
        "    {\n",
        "        \"Dataset\": k.replace(\"_LR\",\"\").replace(\"_NB\",\"\"),\n",
        "        \"Model\": v[\"model\"],\n",
        "        \"Accuracy\": v[\"accuracy\"],\n",
        "        \"Precision\": v[\"precision\"],\n",
        "        \"Recall\": v[\"recall\"],\n",
        "        \"F1-score\": v[\"f1\"]\n",
        "    }\n",
        "    for k, v in results.items()\n",
        "] + [\n",
        "    {\n",
        "        \"Dataset\": k,\n",
        "        \"Model\": \"BERT\",\n",
        "        \"Accuracy\": v[\"accuracy\"],\n",
        "        \"Precision\": v[\"precision\"],\n",
        "        \"Recall\": v[\"recall\"],\n",
        "        \"F1-score\": v[\"f1\"]\n",
        "    }\n",
        "    for k, v in results_bert.items()\n",
        "])\n",
        "print(\"\\n========== Summary Table ==========\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bgRFm4JkKBQ",
        "outputId": "5c9c1a8d-c144-402c-df75-c8358d4c658e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Summary Table ==========\n",
            "   Dataset                Model  Accuracy  Precision    Recall  F1-score\n",
            "0    Movie  Logistic Regression      0.65   1.000000  0.222222  0.363636\n",
            "1    Movie          Naive Bayes      0.60   1.000000  0.111111  0.200000\n",
            "2  Product  Logistic Regression      0.55   0.600000  0.300000  0.400000\n",
            "3  Product          Naive Bayes      0.50   0.500000  0.400000  0.444444\n",
            "4   Tweets  Logistic Regression      0.60   0.583333  0.700000  0.636364\n",
            "5   Tweets          Naive Bayes      0.70   0.750000  0.600000  0.666667\n",
            "6    Movie                 BERT      0.85   1.000000  0.666667  0.800000\n",
            "7  Product                 BERT      0.65   0.714286  0.500000  0.588235\n",
            "8   Tweets                 BERT      0.90   0.900000  0.900000  0.900000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation and Comparison Analysis**\n",
        "\n",
        "The results show a clear performance difference between classical machine learning models and the BERT-based transformer model across all three datasets.\n",
        "\n",
        "**Movie Reviews**\n",
        "\n",
        "For movie reviews, both Logistic Regression and Naive Bayes achieve high precision but very low recall. This indicates that while the models correctly predict positive sentiment when confident, they miss many positive examples. In contrast, BERT significantly improves this balance, achieving higher accuracy and F1-score by effectively capturing contextual information in longer reviews.\n",
        "\n",
        "**Product Reviews**\n",
        "\n",
        "In the product reviews dataset, classical models demonstrate moderate performance and struggle with mixed or subtle sentiment expressions. BERT again outperforms the classical approaches, showing improved accuracy and a higher F1-score, which suggests a stronger ability to understand nuanced opinions.\n",
        "\n",
        "**Tweets**\n",
        "\n",
        "Tweets often contain informal language and limited context, making sentiment classification more challenging. Classical models show inconsistent performance, with Naive Bayes slightly outperforming Logistic Regression. However, BERT clearly achieves the best results, delivering the highest accuracy along with well-balanced precision and recall.\n",
        "\n",
        "**Overall Comparison**\n",
        "\n",
        "Overall, the results confirm that while classical machine learning models are computationally efficient and interpretable, they are limited in handling contextual and informal language. The BERT-based model consistently provides superior sentiment classification performance due to its ability to capture contextual meaning in text.\n"
      ],
      "metadata": {
        "id": "sTgrwOd1mMkQ"
      }
    }
  ]
}